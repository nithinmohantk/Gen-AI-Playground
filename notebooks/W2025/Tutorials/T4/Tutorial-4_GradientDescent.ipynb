{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "### Required Setup\n",
    "\n",
    "If you don't have Python on your computer, you can use the [Anaconda Python distribution](http://continuum.io/downloads) to install most of the Python packages you need. Anaconda provides a simple double-click installer for your convenience.\n",
    "\n",
    "This notebook uses several Python packages that come standard with the Anaconda Python distribution. The primary libraries that we'll be using are:\n",
    "\n",
    "* **NumPy**: Provides a fast numerical array structure and helper functions.\n",
    "* **pandas**: Provides a DataFrame structure to store data in memory and work with it easily and efficiently.\n",
    "* **scikit-learn/sklearn**: The essential Machine Learning package in Python.\n",
    "* **matplotlib**: Basic plotting library in Python; most other Python plotting libraries are built on top of it.\n",
    "* **Seaborn**: Advanced statistical plotting library.\n",
    "* **waterqmark**: A Jupyter Notebook extension for printing timestamps, version numbers, and hardware information.\n",
    "\n",
    "To make sure you have all of the packages you need, install them with `conda`:\n",
    "\n",
    "```\n",
    "conda create -n SYSC4415_tutorials python=3.11\n",
    "conda activate SYSC4415_tutorials\n",
    "\n",
    "conda install jupyter\n",
    "conda install numpy pandas scikit-learn matplotlib seaborn graphviz statsmodels\n",
    "conda install -c conda-forge watermark\n",
    "\n",
    "```\n",
    "\n",
    "`conda` may ask you to update some of them if you don't have the most recent version. Allow it to do so.\n",
    "\n",
    "**Note:** I will not support people trying to run this notebook outside of the Anaconda Python distribution.\n",
    "\n",
    "\n",
    "### _New Stuff to Install for this Tutorial_:\n",
    "\n",
    "```\n",
    "conda install sympy\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "# Tutorial 4 - Gradient Descent\n",
    "\n",
    "**Course:** SYSC 4415 - Introduction to Machine Learning\n",
    "\n",
    "**Semester:** Winter 2025\n",
    "\n",
    "**Adapted by:** [Kevin Dick](https://kevindick.ai/), [Igor Bogdanov](igorbogdanov@cmail.carleton.ca) \n",
    "\n",
    "**Adapted From:** [Daniel Newman](https://github.com/dtnewman)'s [Notebook](https://nbviewer.jupyter.org/github/dtnewman/gradient_descent/blob/master/stochastic_gradient_descent.ipynb)\n",
    "\n",
    "and [Gradient Descent Notebook](https://github.com/udlbook/udlbook/blob/main/Notebooks/Chap06/6_2_Gradient_Descent.ipynb) from [Understanding Deep Learning](https://udlbook.github.io/udlbook/) by [Simon J.D. Prince](http://udlbook.com)\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we will:\n",
    "1. define gradient descent (GD)\n",
    "2. visualize GD applied to an arbitrary function\n",
    "3. use GD to fit a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQGSEOdRlipg"
   },
   "source": [
    "## 1. Defining Gradient Descent\n",
    "\n",
    "**Gradient descent**, also known as <b>steepest descent</b>, is an optimization algorithm for finding the local minimum of a function. \n",
    "\n",
    "**Assumption:** the function must be differentiable.\n",
    "\n",
    "To find a local minimum, the function _\"steps\"_ in the  direction of the negative of the gradient. <b>Gradient ascent</b> is the same as gradient descent, except that it steps in the direction of the positive of the gradient and therefore finds local maximums instead of minimums. \n",
    "\n",
    "The algorithm of gradient descent can be outlined as follows:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 1: &nbsp; Choose initial guess $x_0$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    2: &nbsp; <b>for</b> k = 0, 1, 2, ... <b>do</b> <br>\n",
    "&nbsp;&nbsp;&nbsp;    3:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s_k$ = -$\\nabla f(x_k)$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    4:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $x_{k+1} = x_k + \\alpha_k s_k$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    5: &nbsp;  <b>end for</b>\n",
    "\n",
    "\n",
    "## 2. Visualizing Gradient Descent\n",
    "\n",
    "As a simple example, let's find a local minimum for the function $f(x) = x^3-2x^2+2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvALfjZ3V_UO"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sympy import *\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy.optimize import fmin\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def plot_curve(x_list, y_list):\n",
    "  \"\"\" plot_curve\n",
    "      Helper function to iteratively plot\n",
    "      arbitrary curves.\n",
    "  \"\"\"\n",
    "  titles = [\"Gradient descent\", \n",
    "            \"Gradient descent (zoomed in)\", \n",
    "            \"Gradient descent (zoomed most)\"]\n",
    "  xlims = [[-1,2.5],\n",
    "           [1.2,2.1],\n",
    "           [1.32,1.34]]\n",
    "  plt.figure(figsize=[10,3])\n",
    "  for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.scatter(x_list,y_list,c=\"r\")\n",
    "    plt.plot(x_list,y_list,c=\"r\")\n",
    "    plt.plot(x,f(x), c=\"b\")\n",
    "    plt.xlim(xlims[i])\n",
    "    plt.ylim([0,3])\n",
    "    plt.title(titles[i])\n",
    "  plt.show()\n",
    "\n",
    "def plot_crickets(data, theta_new=None):\n",
    "  \"\"\" plot_curve\n",
    "      Helper function to iteratively plot\n",
    "      arbitrary curves.\n",
    "      ---\n",
    "      theta_new, if defined, is a two-tuple of estimated parameters.\n",
    "  \"\"\"\n",
    "  xx = np.linspace(0,21,1000)\n",
    "  plt.scatter(data[:, 0], data[:, 1], marker='o', c='b')\n",
    "  if not theta_new is None: plt.plot(xx,h(theta_new[0],theta_new[1],xx))\n",
    "  plt.title('Striped Ground Cricket Chirp Frequency vs. Temperature')\n",
    "  plt.xlabel('Frequency (chirps/sec)')\n",
    "  plt.ylabel('Temperature (F)')\n",
    "  plt.xlim([13,21])\n",
    "  plt.ylim([65,95])\n",
    "  plt.show()\n",
    "\n",
    "def plot_generic(data, theta_new=None):\n",
    "  \"\"\" plot_generic\n",
    "      Helper function to iteratively plot\n",
    "      arbitrary curves.\n",
    "      ---\n",
    "      theta_new, if defined, is a two-tuple of estimated parameters.\n",
    "  \"\"\"\n",
    "  xx = np.linspace(0,100,1000)\n",
    "  plt.scatter(data[:, 0], data[:, 1], marker='o', c='b', alpha=0.1)\n",
    "  if not theta_new is None: plt.plot(xx,h(theta_new[0],theta_new[1],xx), c=\"r\", zorder=999)\n",
    "  plt.xlabel('x0')\n",
    "  plt.ylabel('x1')\n",
    "  plt.title(f'num. points: {len(data[:, 0])}')\n",
    "  plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KM0o2mMjV_UV"
   },
   "outputs": [],
   "source": [
    "# Explicitly: We can create our function and derivative explicitly with lambdas\n",
    "# our function\n",
    "f = lambda x: x**3-2*x**2+2\n",
    "\n",
    "# the derivative of our function\n",
    "f_prime = lambda x: 3*x**2-4*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TelhdBQhoon5"
   },
   "outputs": [],
   "source": [
    "# Alternatively: We can use the SymPy library to do this for us\n",
    "# We have to create a \"symbol\" called x\n",
    "X = Symbol('x')\n",
    "f = X**3-2*X**2+2\n",
    "f_prime = f.diff(X)\n",
    "\n",
    "# We can then automatically turn them into labmda functions\n",
    "f = lambdify(X, f)\n",
    "f_prime = lambdify(X, f_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "tfkTBjMEV_UZ",
    "outputId": "d3e1ca6a-a27d-4033-85f5-5cdc4c584180"
   },
   "outputs": [],
   "source": [
    "# Plot the function\n",
    "x = np.linspace(-1,2.5,1000)\n",
    "plt.plot(x,f(x))\n",
    "plt.xlim([-1,2.5])\n",
    "plt.ylim([0,3])\n",
    "plt.ylabel('f(x)')\n",
    "plt.xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKazUy6qV_Uc"
   },
   "source": [
    "We can see from plot above that our local minimum is gonna be near around 1.4 or 1.5 (on the x-axis), but let's pretend that we don't know that, so we set our starting point (arbitrarily, in this case) at $x_0 = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "pseCBf1KV_Ud",
    "outputId": "c3364fe4-e4d6-4c81-84d1-bcce83696511"
   },
   "outputs": [],
   "source": [
    "# Setup some variables\n",
    "x_old     = 0       # Initialized for plotting\n",
    "x_new     = 2       # The algorithm starts at x = 2\n",
    "n_k       = 0.1     # step size\n",
    "precision = 0.0001  # threshold to stop iterations\n",
    "delay     = 1       # one second of sleep to visualiize\n",
    "\n",
    "# Lists to track values to plot\n",
    "x_list, y_list = [x_new], [f(x_new)]\n",
    " \n",
    "while abs(x_new - x_old) > precision:\n",
    "  # Display output each iteration\n",
    "  clear_output(wait=True)\n",
    "  print(f'Iteration {len(x_list)}\\tx: {x_new}\\tdiff: {abs(x_new - x_old)}')\n",
    "  plot_curve(x_list, y_list)\n",
    "  sleep(delay) # To visualize\n",
    "\n",
    "  # Apply Gradient Descent\n",
    "  x_old = x_new\n",
    "  s_k = -f_prime(x_old)\n",
    "  x_new = x_old + n_k * s_k\n",
    "  x_list.append(x_new)\n",
    "  y_list.append(f(x_new))\n",
    "\n",
    "# Print the x-value at the local minimum\n",
    "print('Local minimum occurs at: {:.3f}'.format(x_new))\n",
    "print(\"Number of steps:\", len(x_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMH46GBqV_Up"
   },
   "source": [
    "### Constant Step Size & [Google's Toy Example](https://developers.google.com/machine-learning/crash-course/fitter/graph)\n",
    "You'll notice that the step size (also called **learning rate**) in the implementation above is **constant**. Doing this makes it easier to implement the algorithm. \n",
    "\n",
    "However, it also presents some issues: If the **step size is too small**, then convergence will be **very slow**, but if we make it **too large**, then the method may **fail to converge at all**. \n",
    "\n",
    "### Time-Based Learning Rate Decay Schedule\n",
    "A solution to this is to update the step size is choosing a decrease constant $d$ that shrinks the step size over time (**Time-based**):\n",
    "$\\eta(t+1) = \\frac{\\eta(t)}{(1+t \\times d)}$.\n",
    "\n",
    "Other types of [learning rate schedules](https://en.wikipedia.org/wiki/Learning_rate#Learning_rate_schedule) can also be used, including:\n",
    "* **Step-based** learning schedules changes the learning rate according to some pre-defined steps.\n",
    "* **Exponential** learning schedules are similar to step-based but instead of steps a decreasing exponential function is used.\n",
    "\n",
    "### (Advanced Reading): [\"Why Momentum Really Works\"](https://distill.pub/2017/momentum/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "V8j43FE2V_Uq",
    "outputId": "ff3b7124-9ee5-4699-ee62-534e0cdb3c9f"
   },
   "outputs": [],
   "source": [
    "# Setup the same variables\n",
    "x_old     = 0       # Initialized for plotting\n",
    "x_new     = 2       # The algorithm starts at x = 2\n",
    "n_k       = 0.17    # step size (starting at 0.17 to show effect)\n",
    "precision = 0.0001  # threshold to stop iterations\n",
    "delay     = 1       # one second of sleep to visualiize\n",
    "t,d       = 0,1     # NEW Time-tracking variables and Decay-value\n",
    "\n",
    "# Lists to track values to plot\n",
    "x_list, y_list = [x_new], [f(x_new)]\n",
    " \n",
    "while abs(x_new - x_old) > precision:\n",
    "  # Display output each iteration\n",
    "  clear_output(wait=True)\n",
    "  print(f'Iteration {len(x_list)}\\tx: {x_new}\\tdiff: {abs(x_new - x_old)}')\n",
    "  plot_curve(x_list, y_list)\n",
    "  sleep(delay) # To visualize\n",
    "  \n",
    "  \n",
    "  x_old = x_new\n",
    "  s_k = -f_prime(x_old)\n",
    "  x_new = x_old + n_k * s_k\n",
    "  x_list.append(x_new)\n",
    "  y_list.append(f(x_new))\n",
    "\n",
    "  # Time-based Step Size Decay\n",
    "  n_k = n_k / (1 + t * d)\n",
    "  t += 1\n",
    "\n",
    "print('Local minimum occurs at: {:.3f}'.format(x_new))\n",
    "print(\"Number of steps:\", len(x_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "el8l05WQEO46"
   },
   "source": [
    "# **Gradient descent for Linear Regression**\n",
    "\n",
    "This notebook recreates basic the gradient descent algorithm introduced in [Understanding Deep Learning Book](https://udlbook.github.io/udlbook/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhmIOLiZELV_"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cRkrh9MZ58Z"
   },
   "outputs": [],
   "source": [
    "# Let's create our training data 12 pairs {x_i, y_i}\n",
    "# We'll try to fit the straight line model to these data\n",
    "data = np.array([[0.03,0.19,0.34,0.46,0.78,0.81,1.08,1.18,1.39,1.60,1.65,1.90],\n",
    "                 [0.67,0.85,1.05,1.00,1.40,1.50,1.30,1.54,1.55,1.68,1.73,1.60]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQUERmb2erAe"
   },
   "outputs": [],
   "source": [
    "# Let's define our model -- just a straight line with intercept phi[0] and slope phi[1]\n",
    "def model(phi,x):\n",
    "  y_pred = phi[0]+phi[1] * x\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFRe9POHF2le"
   },
   "outputs": [],
   "source": [
    "# Draw model\n",
    "def draw_model(data,model,phi,title=None):\n",
    "  x_model = np.arange(0,2,0.01)\n",
    "  y_model = model(phi,x_model)\n",
    "\n",
    "  fix, ax = plt.subplots()\n",
    "  ax.plot(data[0,:],data[1,:],'bo')\n",
    "  ax.plot(x_model,y_model,'m-')\n",
    "  ax.set_xlim([0,2]);ax.set_ylim([0,2])\n",
    "  ax.set_xlabel('x'); ax.set_ylabel('y')\n",
    "  ax.set_aspect('equal')\n",
    "  if title is not None:\n",
    "    ax.set_title(title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXx1Tpd1Tl-I"
   },
   "outputs": [],
   "source": [
    "# Initialize the parameters to some arbitrary values and draw the model\n",
    "phi = np.zeros((2,1))\n",
    "phi[0] = 0.6      # Intercept\n",
    "phi[1] = -0.2      # Slope\n",
    "draw_model(data,model,phi, \"Initial parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU5mdGvpTtEG"
   },
   "source": [
    "Now let's compute the sum of squares loss for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7dqTY2Gg7CR"
   },
   "outputs": [],
   "source": [
    "def compute_loss(data_x, data_y, model, phi):\n",
    "  # First make model predictions from data x\n",
    "  # Then compute the squared difference between the predictions and true y values\n",
    "  # Then sum them all and return\n",
    "  pred_y = model(phi,data_x)\n",
    "  sq_diff = (pred_y - data_y)**2\n",
    "  loss = np.sum(sq_diff)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ty05UtEEg9tc"
   },
   "outputs": [],
   "source": [
    "loss = compute_loss(data[0,:],data[1,:],model,np.array([[0.6],[-0.2]]))\n",
    "print('Calculated loss = %3.3f'%(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3trnavPiHpH"
   },
   "source": [
    "Now let's plot the whole loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-NTHpAAHlCl"
   },
   "outputs": [],
   "source": [
    "def draw_loss_function(compute_loss, data,  model, phi_iters = None):\n",
    "  # Define pretty colormap\n",
    "  my_colormap_vals_hex =('2a0902', '2b0a03', '2c0b04', '2d0c05', '2e0c06', '2f0d07', '300d08', '310e09', '320f0a', '330f0b', '34100b', '35110c', '36110d', '37120e', '38120f', '39130f', '3a1410', '3b1411', '3c1511', '3d1612', '3e1613', '3f1713', '401714', '411814', '421915', '431915', '451a16', '461b16', '471b17', '481c17', '491d18', '4a1d18', '4b1e19', '4c1f19', '4d1f1a', '4e201b', '50211b', '51211c', '52221c', '53231d', '54231d', '55241e', '56251e', '57261f', '58261f', '592720', '5b2821', '5c2821', '5d2922', '5e2a22', '5f2b23', '602b23', '612c24', '622d25', '632e25', '652e26', '662f26', '673027', '683027', '693128', '6a3229', '6b3329', '6c342a', '6d342a', '6f352b', '70362c', '71372c', '72372d', '73382e', '74392e', '753a2f', '763a2f', '773b30', '783c31', '7a3d31', '7b3e32', '7c3e33', '7d3f33', '7e4034', '7f4134', '804235', '814236', '824336', '834437', '854538', '864638', '874739', '88473a', '89483a', '8a493b', '8b4a3c', '8c4b3c', '8d4c3d', '8e4c3e', '8f4d3f', '904e3f', '924f40', '935041', '945141', '955242', '965343', '975343', '985444', '995545', '9a5646', '9b5746', '9c5847', '9d5948', '9e5a49', '9f5a49', 'a05b4a', 'a15c4b', 'a35d4b', 'a45e4c', 'a55f4d', 'a6604e', 'a7614e', 'a8624f', 'a96350', 'aa6451', 'ab6552', 'ac6552', 'ad6653', 'ae6754', 'af6855', 'b06955', 'b16a56', 'b26b57', 'b36c58', 'b46d59', 'b56e59', 'b66f5a', 'b7705b', 'b8715c', 'b9725d', 'ba735d', 'bb745e', 'bc755f', 'bd7660', 'be7761', 'bf7862', 'c07962', 'c17a63', 'c27b64', 'c27c65', 'c37d66', 'c47e67', 'c57f68', 'c68068', 'c78169', 'c8826a', 'c9836b', 'ca846c', 'cb856d', 'cc866e', 'cd876f', 'ce886f', 'ce8970', 'cf8a71', 'd08b72', 'd18c73', 'd28d74', 'd38e75', 'd48f76', 'd59077', 'd59178', 'd69279', 'd7937a', 'd8957b', 'd9967b', 'da977c', 'da987d', 'db997e', 'dc9a7f', 'dd9b80', 'de9c81', 'de9d82', 'df9e83', 'e09f84', 'e1a185', 'e2a286', 'e2a387', 'e3a488', 'e4a589', 'e5a68a', 'e5a78b', 'e6a88c', 'e7aa8d', 'e7ab8e', 'e8ac8f', 'e9ad90', 'eaae91', 'eaaf92', 'ebb093', 'ecb295', 'ecb396', 'edb497', 'eeb598', 'eeb699', 'efb79a', 'efb99b', 'f0ba9c', 'f1bb9d', 'f1bc9e', 'f2bd9f', 'f2bfa1', 'f3c0a2', 'f3c1a3', 'f4c2a4', 'f5c3a5', 'f5c5a6', 'f6c6a7', 'f6c7a8', 'f7c8aa', 'f7c9ab', 'f8cbac', 'f8ccad', 'f8cdae', 'f9ceb0', 'f9d0b1', 'fad1b2', 'fad2b3', 'fbd3b4', 'fbd5b6', 'fbd6b7', 'fcd7b8', 'fcd8b9', 'fcdaba', 'fddbbc', 'fddcbd', 'fddebe', 'fddfbf', 'fee0c1', 'fee1c2', 'fee3c3', 'fee4c5', 'ffe5c6', 'ffe7c7', 'ffe8c9', 'ffe9ca', 'ffebcb', 'ffeccd', 'ffedce', 'ffefcf', 'fff0d1', 'fff2d2', 'fff3d3', 'fff4d5', 'fff6d6', 'fff7d8', 'fff8d9', 'fffada', 'fffbdc', 'fffcdd', 'fffedf', 'ffffe0')\n",
    "  my_colormap_vals_dec = np.array([int(element,base=16) for element in my_colormap_vals_hex])\n",
    "  r = np.floor(my_colormap_vals_dec/(256*256))\n",
    "  g = np.floor((my_colormap_vals_dec - r *256 *256)/256)\n",
    "  b = np.floor(my_colormap_vals_dec - r * 256 *256 - g * 256)\n",
    "  my_colormap = ListedColormap(np.vstack((r,g,b)).transpose()/255.0)\n",
    "\n",
    "  # Make grid of intercept/slope values to plot\n",
    "  intercepts_mesh, slopes_mesh = np.meshgrid(np.arange(0.0,2.0,0.02), np.arange(-1.0,1.0,0.002))\n",
    "  loss_mesh = np.zeros_like(slopes_mesh)\n",
    "  # Compute loss for every set of parameters\n",
    "  for idslope, slope in np.ndenumerate(slopes_mesh):\n",
    "     loss_mesh[idslope] = compute_loss(data[0,:], data[1,:], model, np.array([[intercepts_mesh[idslope]], [slope]]))\n",
    "\n",
    "  fig,ax = plt.subplots()\n",
    "  fig.set_size_inches(8,8)\n",
    "  ax.contourf(intercepts_mesh,slopes_mesh,loss_mesh,256,cmap=my_colormap)\n",
    "  ax.contour(intercepts_mesh,slopes_mesh,loss_mesh,40,colors=['#80808080'])\n",
    "  if phi_iters is not None:\n",
    "    ax.plot(phi_iters[0,:], phi_iters[1,:],'go-')\n",
    "  ax.set_ylim([1,-1])\n",
    "  ax.set_xlabel('Intercept $\\phi_{0}$'); ax.set_ylabel('Slope, $\\phi_{1}$')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8HbvIupnTME",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_loss_function(compute_loss, data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9Duf05WqqSC"
   },
   "source": [
    "Now let's compute the gradient vector for a given set of parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial \\boldsymbol\\phi} = \\begin{bmatrix}\\frac{\\partial L}{\\partial \\phi_0} \\\\\\frac{\\partial L}{\\partial \\phi_1} \\end{bmatrix}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpswmkL2qwBT"
   },
   "outputs": [],
   "source": [
    "# Expressions for the sum of squares loss and take the\n",
    "# derivative with respect to phi0 and phi1\n",
    "def compute_gradient(data_x, data_y, phi):\n",
    "    \n",
    "    dl_dphi0 = np.sum(2*(phi[0] + phi[1]*data_x - data_y))\n",
    "    dl_dphi1 = np.sum(2*data_x*(phi[0] + phi[1]*data_x - data_y))\n",
    "    \n",
    "    # Return the gradient\n",
    "    return np.array([[dl_dphi0],[dl_dphi1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RS1nEcYVuEAM"
   },
   "source": [
    "We can check we got this right using a trick known as **finite differences**.  If we evaluate the function and then change one of the parameters by a very small amount and normalize by that amount, we get an approximation to the gradient, so:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\phi_{0}}&\\approx & \\frac{L[\\phi_0+\\delta, \\phi_1]-L[\\phi_0, \\phi_1]}{\\delta}\\\\\n",
    "\\frac{\\partial L}{\\partial \\phi_{1}}&\\approx & \\frac{L[\\phi_0, \\phi_1+\\delta]-L[\\phi_0, \\phi_1]}{\\delta}\n",
    "\\end{align}\n",
    "\n",
    "We can't do this when there are many parameters;  for a million parameters, we would have to evaluate the loss function one million plus one times, and usually computing the gradients directly is much more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuwAHN7yt-gi"
   },
   "outputs": [],
   "source": [
    "# Compute the gradient using your function\n",
    "gradient = compute_gradient(data[0,:],data[1,:], phi)\n",
    "print(f\"Computed gradient: ({gradient[0].item():.3f},{gradient[1].item():.3f})\")\n",
    "# Approximate the gradients with finite differences\n",
    "delta = 0.0001\n",
    "dl_dphi0_est = (compute_loss(data[0,:],data[1,:],model,phi+np.array([[delta],[0]])) - \\\n",
    "                    compute_loss(data[0,:],data[1,:],model,phi))/delta\n",
    "dl_dphi1_est = (compute_loss(data[0,:],data[1,:],model,phi+np.array([[0],[delta]])) - \\\n",
    "                    compute_loss(data[0,:],data[1,:],model,phi))/delta\n",
    "print(f\"Approx gradients: ({dl_dphi0_est:.3f},{dl_dphi1_est:.3f})\")\n",
    "# There might be small differences in the last significant figure because finite gradients is an approximation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EIjMM9Fw2eT"
   },
   "source": [
    "Now we are ready to perform gradient descent.  We'll use line search plus the helper function loss_function_1D that maps the search along the negative gradient direction in 2D space to a 1D problem (distance along this direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrJ2gQjfw1XP"
   },
   "outputs": [],
   "source": [
    "def loss_function_1D(dist_prop, data, model, phi_start, search_direction):\n",
    "  # Return the loss after moving this far\n",
    "  return compute_loss(data[0,:], data[1,:], model, phi_start+ search_direction * dist_prop)\n",
    "\n",
    "def line_search(data, model, phi, gradient, thresh=.00001, max_dist = 0.1, max_iter = 15, verbose=False):\n",
    "    # Initialize four points along the range we are going to search\n",
    "    a = 0\n",
    "    b = 0.33 * max_dist\n",
    "    c = 0.66 * max_dist\n",
    "    d = 1.0 * max_dist\n",
    "    n_iter = 0\n",
    "\n",
    "    # While we haven't found the minimum closely enough\n",
    "    while np.abs(b-c) > thresh and n_iter < max_iter:\n",
    "        # Increment iteration counter (just to prevent an infinite loop)\n",
    "        n_iter = n_iter+1\n",
    "        # Calculate all four points\n",
    "        lossa = loss_function_1D(a, data, model, phi,gradient)\n",
    "        lossb = loss_function_1D(b, data, model, phi,gradient)\n",
    "        lossc = loss_function_1D(c, data, model, phi,gradient)\n",
    "        lossd = loss_function_1D(d, data, model, phi,gradient)\n",
    "\n",
    "        if verbose:\n",
    "          print('Iter %d, a=%3.3f, b=%3.3f, c=%3.3f, d=%3.3f'%(n_iter, a,b,c,d))\n",
    "          print('a %f, b%f, c%f, d%f'%(lossa,lossb,lossc,lossd))\n",
    "\n",
    "        # Rule #1 If point A is less than points B, C, and D then halve distance from A to points B,C, and D\n",
    "        if np.argmin((lossa,lossb,lossc,lossd))==0:\n",
    "          b = a+ (b-a)/2\n",
    "          c = a+ (c-a)/2\n",
    "          d = a+ (d-a)/2\n",
    "          continue;\n",
    "\n",
    "        # Rule #2 If point b is less than point c then\n",
    "        #                     point d becomes point c, and\n",
    "        #                     point b becomes 1/3 between a and new d\n",
    "        #                     point c becomes 2/3 between a and new d\n",
    "        if lossb < lossc:\n",
    "          d = c\n",
    "          b = a+ (d-a)/3\n",
    "          c = a+ 2*(d-a)/3\n",
    "          continue\n",
    "\n",
    "        # Rule #2 If point c is less than point b then\n",
    "        #                     point a becomes point b, and\n",
    "        #                     point b becomes 1/3 between new a and d\n",
    "        #                     point c becomes 2/3 between new a and d\n",
    "        a = b\n",
    "        b = a+ (d-a)/3\n",
    "        c = a+ 2*(d-a)/3\n",
    "\n",
    "    # Return average of two middle points\n",
    "    return (b+c)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVq6rmaWRD2M"
   },
   "outputs": [],
   "source": [
    "def gradient_descent_step(phi, data,  model):\n",
    "  # 1. Compute the gradient (you wrote this function above)\n",
    "  \n",
    "  data_x = data[0,:]\n",
    "  data_y = data[1,:]\n",
    "    \n",
    "  gradient = compute_gradient(data_x,data_y, phi)\n",
    "\n",
    "  print(f\"gradient: {gradient} \")\n",
    "\n",
    "  # 2. Find the best step size alpha using line search function (above) -- use negative gradient as going downhill\n",
    "  alpha = line_search(data, model, phi, gradient, thresh=.04, max_dist = 2, max_iter = 120, verbose=False)\n",
    "\n",
    "  print(f\"Alpha: {alpha}\")  \n",
    "  \n",
    "  # 3. Update the parameters phi based on the gradient and the step size alpha.\n",
    "  phi = phi - alpha* gradient\n",
    "  print(f\"PHI: {phi}\")\n",
    "  return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOLd0gtdRLLS"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Initialize the parameters and draw the model\n",
    "n_steps = 100\n",
    "phi_all = np.zeros((2,n_steps+1))\n",
    "phi_all[0,0] = 1.6\n",
    "phi_all[1,0] = -0.5\n",
    "\n",
    "# Measure loss and draw initial model\n",
    "loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,0:1])\n",
    "draw_model(data,model,phi_all[:,0:1], \"Initial parameters, Loss = %f\"%(loss))\n",
    "\n",
    "# Repeatedly take gradient descent steps\n",
    "for c_step in range (n_steps):\n",
    "  print(f\"step:{c_step}\")\n",
    "  # Do gradient descent step\n",
    "  print(f\"current phi_all:{phi_all[:,c_step:c_step+1]}\")\n",
    "  phi_all[:,c_step+1:c_step+2] = gradient_descent_step(phi_all[:,c_step:c_step+1],data, model)\n",
    "  # Measure loss and draw model\n",
    "  loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,c_step+1:c_step+2])\n",
    "  clear_output(wait=True)\n",
    "  draw_model(data,model,phi_all[:,c_step+1], \"Iteration %d, loss = %f\"%(c_step+1,loss))\n",
    "\n",
    "# Draw the trajectory on the loss function\n",
    "draw_loss_function(compute_loss, data, model,phi_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0nGBbpZV_Ut"
   },
   "source": [
    "## Using Gradient Descent to Fit a Linear Regression Model\n",
    "Let's now consider an example which is a little bit more complicated. \n",
    "\n",
    "Consider a simple **linear regression** where we want to see how the temperature affects the noises made by crickets. \n",
    "\n",
    "We have a data set of cricket chirp rates at various temperatures. First we'll load that data set in and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "2WsJ80__V_Uu",
    "outputId": "e2fedeb0-7566-41cc-895c-73fa9ad2bbc2"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = np.loadtxt('https://github.com/dtnewman/gradient_descent/raw/master/SGD_data.txt', delimiter=',') #Using \"raw\" URL from github repository\n",
    "\n",
    "#print(data)\n",
    "x = data[:, 0] # Everything from the 0th column\n",
    "y = data[:, 1] # Everything from the 1st column\n",
    "m = len(x)     # Number of points\n",
    "\n",
    "# Plot the data\n",
    "plot_crickets(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aySjsl3cV_Ux"
   },
   "source": [
    "Our goal is to find the equation of the straight line $h_\\theta(x) = \\theta_0 + \\theta_1 x$ that best fits our data points. \n",
    "\n",
    "The function that we are trying to minimize in this case is:\n",
    "\n",
    "$J(\\theta_0,\\theta_1) = {1 \\over 2m} \\sum\\limits_{i=1}^m (h_\\theta(x_i)-y_i)^2$\n",
    "\n",
    "In this case, our gradient will be defined in two dimensions. We must take the derivative with respect to each of the model parameters we wish to estimate, $\\theta_0$ and $\\theta_1$:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0,\\theta_1) = \\frac{1}{m}  \\sum\\limits_{i=1}^m (h_\\theta(x_i)-y_i)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0,\\theta_1) = \\frac{1}{m}  \\sum\\limits_{i=1}^m ((h_\\theta(x_i)-y_i) \\cdot x_i)$\n",
    "\n",
    "Below, we set up our function for $h$, $J$ and the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4gr032jV_Uy"
   },
   "outputs": [],
   "source": [
    "h = lambda theta_0,theta_1,x: theta_0 + theta_1*x\n",
    "\n",
    "def J(x,y,m,theta_0,theta_1):\n",
    "    returnValue = 0\n",
    "    for i in range(m):\n",
    "        returnValue += (h(theta_0,theta_1,x[i])-y[i])**2\n",
    "    returnValue = returnValue/(2*m)\n",
    "    return returnValue\n",
    "\n",
    "def grad_J(x,y,m,theta_0,theta_1):\n",
    "    returnValue = np.array([0.,0.])\n",
    "    for i in range(m):\n",
    "        returnValue[0] += (h(theta_0,theta_1,x[i])-y[i])\n",
    "        returnValue[1] += (h(theta_0,theta_1,x[i])-y[i])*x[i]\n",
    "    returnValue = returnValue/(m)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1kSw0QIV_U4"
   },
   "source": [
    "And we run our gradient descent algorithm (without adaptive step sizes in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "qefrVXpXV_U5",
    "outputId": "66fa967f-12ef-428c-c23f-3946408f545b"
   },
   "outputs": [],
   "source": [
    "theta_old = np.array([0.,0.])\n",
    "theta_new = np.array([1.,1.]) # The algorithm starts at [1,1]\n",
    "n_k       = 0.001 # step size\n",
    "precision = 0.001\n",
    "num_steps = 0\n",
    "s_k       = float(\"inf\")\n",
    "delay    = 0.1\n",
    "\n",
    "while np.linalg.norm(s_k) > precision:\n",
    "  #\"\"\" Toggle to comment out this plotting block (HINT: This converges around 500K...)\n",
    "  clear_output(wait=True)\n",
    "  print(f'Iteration {num_steps}\\tx: {x_new}\\tdiff: {np.linalg.norm(s_k)}')\n",
    "  plot_crickets(data, theta_new)\n",
    "  sleep(delay) # To visualize\"\"\"\n",
    "\n",
    "  # Apply Gradient Descent\n",
    "  num_steps += 1\n",
    "  theta_old = theta_new\n",
    "  s_k = -grad_J(x,y,m,theta_old[0],theta_old[1])\n",
    "  theta_new = theta_old + n_k * s_k\n",
    "\n",
    "print(\"Local minimum occurs where:\")\n",
    "print('theta_0 = {:.3f}'.format(theta_new[0]))\n",
    "print('theta_1 = {:.3f}'.format(theta_new[1]))\n",
    "print(\"This took\",num_steps,\"steps to converge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nsr8Z4o_V_U7"
   },
   "source": [
    "For comparison, let's get the actual values for $\\theta_0$ and $\\theta_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "896WtGTAV_U8",
    "outputId": "4dc30725-c92a-488e-baa6-5750dd495fe4"
   },
   "outputs": [],
   "source": [
    "actual_vals = sp.stats.linregress(x,y)\n",
    "print(f\"Actual values for theta (using built-in linear regression) are:\\ntheta_0 = {actual_vals.intercept}\\ntheta_1 = {actual_vals.slope}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOyn0MCuV_VB"
   },
   "source": [
    "So we see that our values are relatively close to the actual values (however our method was incredibly slow). \n",
    "\n",
    "If you look at the source code of [linregress](https://github.com/scipy/scipy/blob/master/scipy/stats/_stats_mstats_common.py), it uses the **convariance matrix of x and y** to rapidly compute the next parameter values.\n",
    "\n",
    "#### Every Single Point is Being Used Each Round!\n",
    "Notice that in the method above we need to **calculate the gradient in every step** of our algorithm. \n",
    "\n",
    "In the example with the crickets, this is not a big deal since there are **only 15 data points**. But imagine that we had 10 million data points! If this were the case, it would certainly make the method above far less efficient.\n",
    "\n",
    "---\n",
    "\n",
    "#### (Optional) Applied Homework: Implement Mini-Batch Gradient Descent\n",
    "In machine learning, the algorithm above is often called <b>batch gradient descent</b> to contrast it with <b>mini-batch gradient descent</b> (which we will not go into here) and <b>stochastic gradient descent</b>.\n",
    "\n",
    "**Mini-batch gradient descent** is a variation of the gradient descent algorithm that **splits the training dataset into small batches** that are used to calculate model error and update model coefficients.\n",
    "\n",
    "Implementations may choose to **sum the gradient** over the mini-batch which further reduces the variance of the gradient.\n",
    "\n",
    "Mini-batch gradient descent seeks to find **a balance between the robustness** of **stochastic gradient descent** and the **efficiency of batch gradient descent**. It is the most common implementation of gradient descent used in the field of deep learning.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhLU275RV_VN"
   },
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "As we said above, in batch gradient descent, we must look at every example in the entire training set on every step (in cases where a training set is used for gradient descent). This can be quite slow if the training set is sufficiently large. \n",
    "\n",
    "In **stochastic gradient descent**, we update our values after looking at *each* item in the training set, so that we can **start making progress right away**. \n",
    "\n",
    "Recall the linear regression example above. In that example, we calculated the gradient for each of the two theta values as follows:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0,\\theta_1) = \\frac{1}{m}  \\sum\\limits_{i=1}^m (h_\\theta(x_i)-y_i)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0,\\theta_1) = \\frac{1}{m}  \\sum\\limits_{i=1}^m ((h_\\theta(x_i)-y_i) \\cdot x_i)$\n",
    "\n",
    "Where $h_\\theta(x) = \\theta_0 + \\theta_1 x$\n",
    "\n",
    "Then we followed this algorithm (where $\\alpha$ was a non-adapting stepsize):\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 1: &nbsp; Choose initial guess $x_0$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    2: &nbsp; <b>for</b> k = 0, 1, 2, ... <b>do</b> <br>\n",
    "&nbsp;&nbsp;&nbsp;    3:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s_k$ = -$\\nabla f(x_k)$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    4:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $x_{k+1} = x_k + \\alpha s_k$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    5: &nbsp;  <b>end for</b>\n",
    "\n",
    "When the sample data had 15 data points as in the example above, calculating the gradient was not very costly. But for very large data sets, this would not be the case. So instead, we consider a stochastic gradient descent algorithm for simple linear regression such as the following, where m is the size of the data set:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 1: &nbsp; Randomly shuffle the data set <br>\n",
    "&nbsp;&nbsp;&nbsp;    2: &nbsp; <b>for</b> k = 0, 1, 2, ... <b>do</b> <br>\n",
    "&nbsp;&nbsp;&nbsp;    3: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>for</b> i = 1 to m <b>do</b> <br>\n",
    "&nbsp;&nbsp;&nbsp;     &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Update parameters<br>\n",
    "&nbsp;&nbsp;&nbsp;    4:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\begin{bmatrix}\n",
    " \\theta_{0} \\\\ \n",
    " \\theta_1 \\\\ \n",
    " \\end{bmatrix}=\\begin{bmatrix}\n",
    " \\theta_0 \\\\ \n",
    " \\theta_1 \\\\ \n",
    " \\end{bmatrix}-\\alpha\\begin{bmatrix}\n",
    " 2(h_\\theta(x_i)-y_i) \\\\ \n",
    " 2x_i(h_\\theta(x_i)-y_i) \\\\ \n",
    " \\end{bmatrix}$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    5: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>end for</b> <br> \n",
    "&nbsp;&nbsp;&nbsp;    6: &nbsp;  <b>end for</b>\n",
    "\n",
    "With **batch gradient descent**, we must go through the **entire data set before we make any progress**. With this algorithm though, we can **make progress right away** and continue to make progress as we go through the data set. \n",
    "\n",
    "Therefore, **stochastic gradient descent** is often preferred when dealing with **large data sets.**\n",
    "\n",
    "Typically, with stochastic gradient descent, you will run through the entire data set 1 to 10 times (see value for $k$ in line 2 of the pseudocode above), depending on how fast the data is converging and how large the data set is.\n",
    "\n",
    "Unlike gradient descent, stochastic gradient descent will tend to **oscillate** <i>near</i> a minimum value rather than continuously getting closer. It **may never actually converge to the minimum** though. \n",
    "\n",
    "One way around this is to slowly decrease (**decay**) the step size $\\alpha$ as the algorithm runs. However, this is less common than using a fixed $\\alpha$.\n",
    "\n",
    "Let's look at another example where we illustrate the use of stochastic gradient descent for linear regression. In the example below, we'll create a set of 500,000 points around the line $y = 2x+17+\\epsilon$, for values of x between 0 and 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "9e6KzewtV_VQ",
    "outputId": "b391fca4-eb09-495e-98d7-20f31bde5e61"
   },
   "outputs": [],
   "source": [
    "f = lambda x: x*2+17+np.random.randn(len(x))*10\n",
    "\n",
    "x = np.random.random(500000)*100\n",
    "y = f(x) \n",
    "m = len(y)\n",
    "\n",
    "plot_generic(np.column_stack((x,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxmiPubfV_VV"
   },
   "source": [
    "First, let's randomly shuffle around our dataset. Note that in this example, this step isn't strictly necessary since the data is already in a random order. However, that obviously may not always be the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aF_CKFSmV_Vf"
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "x_shuf = []\n",
    "y_shuf = []\n",
    "index_shuf = list(range(len(x)))\n",
    "shuffle(index_shuf)\n",
    "for i in index_shuf:\n",
    "    x_shuf.append(x[i])\n",
    "    y_shuf.append(y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6O4imNwdV_Vq"
   },
   "source": [
    "Now we'll setup our h function and our cost function, which we will use to check how the value is improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1n9J7gy0V_Vr"
   },
   "outputs": [],
   "source": [
    "h = lambda theta_0,theta_1,x: theta_0 + theta_1*x\n",
    "\n",
    "# Function for calculating COST: The cost function is calculated as an average of loss functions\n",
    "cost = lambda theta_0,theta_1, x_i, y_i: 0.5*(h(theta_0,theta_1,x_i)-y_i)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uK40XjW2V_V2"
   },
   "source": [
    "Now we'll run our stochastic gradient descent algorithm. \n",
    "\n",
    "To see it's progress, we'll take a loss measurement at every step. \n",
    "\n",
    "Every 10,000 steps, we'll get an average cost from the loss measured during the last 10,000 steps and then append that to our cost_list variable. We will run through the entire list 10 times here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "1Yd0CpUjV_V4",
    "outputId": "94911bf5-96ad-4e49-92e5-94bcd304142d"
   },
   "outputs": [],
   "source": [
    "theta_old = np.array([0.,0.])\n",
    "theta_new = np.array([1.,1.]) # The algorithm starts at [1,1]\n",
    "n_k = 0.000005 # step size\n",
    "\n",
    "iter_num = 0\n",
    "s_k = np.array([float(\"inf\"),float(\"inf\")])\n",
    "sum_cost = 0\n",
    "cost_list = []\n",
    "\n",
    "# Num. Plot for faster visualization\n",
    "num_plot = 1000\n",
    "\n",
    "for j in range(10):\n",
    "    for i in range(m):\n",
    "        iter_num += 1\n",
    "        theta_old = theta_new\n",
    "        s_k[0] = (h(theta_old[0],theta_old[1],x[i])-y[i])\n",
    "        s_k[1] = (h(theta_old[0],theta_old[1],x[i])-y[i])*x[i]\n",
    "        s_k = (-1)*s_k\n",
    "        theta_new = theta_old + n_k * s_k\n",
    "        sum_cost += cost(theta_old[0],theta_old[1],x[i],y[i])\n",
    "        if (i+1) % 10000 == 0:\n",
    "            print(f'NOTE: Only plotting a subset of {num_plot} points...')\n",
    "            clear_output(wait=True)\n",
    "            plot_generic(np.column_stack((x[0:num_plot],y[0:num_plot])), theta_new)\n",
    "            print(f'Iteration {j}\\tData Index: {i+1}\\nCost: {sum_cost/10000.0}')\n",
    "            cost_list.append(sum_cost/10000.0)\n",
    "            sum_cost = 0   \n",
    "            \n",
    "print(\"\\nLocal minimum occurs where:\")\n",
    "print('theta_0 = {:.3f}'.format(theta_new[0]))\n",
    "print('theta_1 = {:.3f}'.format(theta_new[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPk7J6lZV_V8"
   },
   "source": [
    "As you can see, our values for $\\theta_0$ and $\\theta_1$ are close to their true values of 17 and 2.\n",
    "\n",
    "Now, we plot our cost versus the number of iterations. As you can see, the cost goes down quickly at first, but starts to level off as we go through more iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "YDqsqI5GV_V9",
    "outputId": "c6347c64-e5e3-4a5a-ce6d-e3500263cebe"
   },
   "outputs": [],
   "source": [
    "iterations = np.arange(len(cost_list))*10000\n",
    "plt.plot(iterations,cost_list)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(f'Model Cost after {len(cost_list)*10000} Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwUuRP_8QlAB"
   },
   "source": [
    "**Question:** Why do we see 10 regular \"peaks\" in the curve above? *(Hint: What did we fail to do at the start of each epoch?)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "4y9XNW94Ic7I",
    "outputId": "a73431d5-ec0e-47a7-9fe4-4b2e2cffcc6e"
   },
   "outputs": [],
   "source": [
    "# The final model plotted over all the datapoints\n",
    "plot_generic(np.column_stack((x,y)), theta_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_REaWatV_WB"
   },
   "source": [
    " \n",
    " ---\n",
    "\n",
    " ## Take Away Messages\n",
    "\n",
    "* **Gradient descent**, also known as <b>steepest descent</b>, is an optimization algorithm for finding the local minimum of a function. \n",
    "* Functions must be **differentiable**.\n",
    "* <b>Gradient ascent</b> is the same as gradient descent, except that it steps in the direction of the **positive of the gradient** and therefore finds **local maximums** instead of minimums. \n",
    "* **Learning rate**) can either be **constant** or **decay** according to a schedule.\n",
    "* Learning Rate Issues: If the **step size is too small**, then convergence will be **very slow**, but if it is **too large**, then the method may **fail to converge at all**. \n",
    "* **Mini-batch gradient descent** is a variation of the gradient descent algorithm that **splits the training dataset into small batches** that are used to calculate model error and update model coefficients.\n",
    "* Mini-batch gradient descent seeks to find **a balance between the robustness** of **stochastic gradient descent** and the **efficiency of batch gradient descent**. It is the most common implementation of gradient descent used in the field of deep learning.\n",
    "* With **batch gradient descent**, we must go through the **entire data set before we make any progress**.\n",
    "* In **stochastic gradient descent**, we update our values after looking at *each* item in the training set, so that we can **start making progress right away**. \n",
    "* Therefore, **stochastic gradient descent** is often preferred when dealing with **large data sets.**\n",
    "* Unlike gradient descent, stochastic gradient descent will tend to **oscillate** <i>near</i> a minimum value rather than continuously getting closer. It **may never actually converge to the minimum** though. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tutorial_4_GradientDescent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
